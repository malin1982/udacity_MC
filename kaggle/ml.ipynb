{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Thank you for opening this script!\n\nI have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n\nPlease **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n\nMy other exploratory studies can be accessed here :\nhttps://www.kaggle.com/sharmasanthosh/kernels\n***\n## Data statistics\n* Shape\n* Peek\n* Description\n* Skew\n\n## Transformation\n* Correction of skew\n\n## Data Interaction\n* Correlation\n* Scatter plot\n\n## Data Visualization\n* Box and density plots\n* Grouping of one hot encoded attributes\n\n## Data Preparation\n* One hot encoding of categorical data\n* Test-train split\n\n## Evaluation, prediction, and analysis\n* Linear Regression (Linear algo)\n* Ridge Regression (Linear algo)\n* LASSO Linear Regression (Linear algo)\n* Elastic Net Regression (Linear algo)\n* KNN (non-linear algo)\n* CART (non-linear algo)\n* SVM (Non-linear algo)\n* Bagged Decision Trees (Bagging)\n* Random Forest (Bagging)\n* Extra Trees (Bagging)\n* AdaBoost (Boosting)\n* Stochastic Gradient Boosting (Boosting)\n* MLP (Deep Learning)\n* XGBoost\n\n## Make Predictions\n***",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Load raw data:\n\nInformation about all the attributes can be found here:\n\nhttps://www.kaggle.com/c/allstate-claims-severity/data\n\nLearning: \nWe need to predict the 'loss' based on the other attributes. Hence, this is a regression problem.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read raw data from the file\n\nimport pandas #provides data structures to quickly analyze data\n#Since this code runs on Kaggle server, data can be accessed directly in the 'input' folder\n#Read the train dataset\ndataset = pandas.read_csv(\"../input/train.csv\") \n\n#Read test dataset\ndataset_test = pandas.read_csv(\"../input/test.csv\")\n#Save the id's for submission file\nID = dataset_test['id']\n#Drop unnecessary columns\ndataset_test.drop('id',axis=1,inplace=True)\n\n#Print all rows and columns. Dont hide any\npandas.set_option('display.max_rows', None)\npandas.set_option('display.max_columns', None)\n\n#Display the first five rows to get a feel of the data\nprint(dataset.head(5))\n\n#Learning : cat1 to cat116 contain alphabets",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data statistics\n* Shape",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Size of the dataframe\n\nprint(dataset.shape)\n\n# We can see that there are 188318 instances having 132 attributes\n\n#Drop the first column 'id' since it just has serial numbers. Not useful in the prediction process.\ndataset = dataset.iloc[:,1:]\n\n#Learning : Data is loaded successfully as dimensions match the data description",
   "execution_count": 2,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data statistics\n* Description",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Statistical description\n\nprint(dataset.describe())\n\n# Learning :\n# No attribute in continuous columns is missing as count is 188318 for all, all rows can be used\n# No negative values are present. Tests such as chi2 can be used\n# Statistics not displayed for categorical data",
   "execution_count": 3,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data statistics\n* Skew",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Skewness of the distribution\n\nprint(dataset.skew())\n\n# Values close to 0 show less ske\n# loss shows the highest skew. Let us visualize it",
   "execution_count": 4,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Visualization\n* Box and density plots",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# We will visualize all the continuous attributes using Violin Plot - a combination of box and density plots\n\nimport numpy\n\n#import plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#range of features considered\nsplit = 116 \n\n#number of features considered\nsize = 15\n\n#create a dataframe with only continuous features\ndata=dataset.iloc[:,split:] \n\n#get the names of all the columns\ncols=data.columns \n\n#Plot violin for all attributes in a 7x2 grid\nn_cols = 2\nn_rows = 7\n\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12, 8))\n    for j in range(n_cols):\n        sns.violinplot(y=cols[i*n_cols+j], data=dataset, ax=ax[j])\n\n\n#cont1 has many values close to 0.5\n#cont2 has a pattern where there a several spikes at specific points\n#cont5 has many values near 0.3\n#cont14 has a distinct pattern. 0.22 and 0.82 have a lot of concentration\n#loss distribution must be converted to normal",
   "execution_count": 5,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Transformation\n* Skew correction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#log1p function applies log(1+x) to all elements of the column\ndataset[\"loss\"] = numpy.log1p(dataset[\"loss\"])\n#visualize the transformed column\nsns.violinplot(data=dataset,y=\"loss\")  \nplt.show()\n\n#Plot shows that skew is corrected to a large extent",
   "execution_count": 6,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Interaction\n* Correlation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Correlation tells relation between two attributes.\n# Correlation requires continous data. Hence, ignore categorical data\n\n# Calculates pearson co-efficient for all combinations\ndata_corr = data.corr()\n\n# Set the threshold to select only highly correlated attributes\nthreshold = 0.5\n\n# List of pairs along with correlation above threshold\ncorr_list = []\n\n#Search for the highly correlated pairs\nfor i in range(0,size): #for 'size' features\n    for j in range(i+1,size): #avoid repetition\n        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n\n#Sort to show higher ones first            \ns_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n\n#Print correlations and column names\nfor v,i,j in s_corr_list:\n    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n\n# Strong correlation is observed between the following pairs\n# This represents an opportunity to reduce the feature set through transformations such as PCA",
   "execution_count": 7,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Interaction\n* Scatter plot",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Scatter plot of only the highly correlated pairs\nfor v,i,j in s_corr_list:\n    sns.pairplot(dataset, size=6, x_vars=cols[i],y_vars=cols[j] )\n    plt.show()\n\n#cont11 and cont12 give an almost linear pattern...one must be removed\n#cont1 and cont9 are highly correlated ...either of them could be safely removed \n#cont6 and cont10 show very good correlation too",
   "execution_count": 8,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Visualization\n* Categorical attributes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Count of each label in each category\n\n#names of all the columns\ncols = dataset.columns\n\n#Plot count plot for all attributes in a 29x4 grid\nn_cols = 4\nn_rows = 29\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n    for j in range(n_cols):\n        sns.countplot(x=cols[i*n_cols+j], data=dataset, ax=ax[j])\n\n#cat1 to cat72 have only two labels A and B. In most of the cases, B has very few entries\n#cat73 to cat 108 have more than two labels\n#cat109 to cat116 have many labels",
   "execution_count": 9,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "##Data Preparation\n* One Hot Encoding of categorical data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas\n\n#cat1 to cat116 have strings. The ML algorithms we are going to study require numberical data\n#One-hot encoding converts an attribute to a binary vector\n\n#Variable to hold the list of variables for an attribute in the train and test data\nlabels = []\n\nfor i in range(0,split):\n    train = dataset[cols[i]].unique()\n    test = dataset_test[cols[i]].unique()\n    labels.append(list(set(train) | set(test)))    \n\ndel dataset_test\n\n#Import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#One hot encode all categorical attributes\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(dataset.iloc[:,i])\n    feature = feature.reshape(dataset.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n\n# Make a 2D array from a list of 1D arrays\nencoded_cats = numpy.column_stack(cats)\n\n# Print the shape of the encoded data\nprint(encoded_cats.shape)\n\n#Concatenate encoded attributes with continuous attributes\ndataset_encoded = numpy.concatenate((encoded_cats,dataset.iloc[:,split:].values),axis=1)\ndel cats\ndel feature\ndel dataset\ndel encoded_cats\nprint(dataset_encoded.shape)",
   "execution_count": 10,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "##Data Preparation\n* Split into train and validation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#get the number of rows and columns\nr, c = dataset_encoded.shape\n\n#create an array which has indexes of columns\ni_cols = []\nfor i in range(0,c-1):\n    i_cols.append(i)\n\n#Y is the target column, X has the rest\nX = dataset_encoded[:,0:(c-1)]\nY = dataset_encoded[:,(c-1)]\ndel dataset_encoded\n\n#Validation chunk size\nval_size = 0.1\n\n#Use a common seed in all experiments so that same chunk is used for validation\nseed = 0\n\n#Split the data into chunks\nfrom sklearn import cross_validation\nX_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\ndel X\ndel Y\n\n#All features\nX_all = []\n\n#List of combinations\ncomb = []\n\n#Dictionary to store the MAE for all algorithms \nmae = []\n\n#Scoring parameter\nfrom sklearn.metrics import mean_absolute_error\n\n#Add this version of X to the list \nn = \"All\"\n#X_all.append([n, X_train,X_val,i_cols])\nX_all.append([n, i_cols])",
   "execution_count": 11,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Linear Regression (Linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of LinearRegression\n\n#Import the library\nfrom sklearn.linear_model import LinearRegression\n\n#uncomment the below lines if you want to run the algo\n##Set the base model\n#model = LinearRegression(n_jobs=-1)\n#algo = \"LR\"\n#\n##Accuracy of the model using all features\n#for name,i_cols_list in X_all:\n#    model.fit(X_train[:,i_cols_list],Y_train)\n#    result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n#    mae.append(result)\n#    print(name + \" %s\" % result)\n#comb.append(algo)\n\n#Result obtained after running the algo. Comment the below two lines if you want to run the algo\nmae.append(1278)\ncomb.append(\"LR\" )    \n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#MAE achieved is 1278",
   "execution_count": 12,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Ridge Regression (Linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of Ridge LinearRegression\n\n#Import the library\nfrom sklearn.linear_model import Ridge\n\n#Add the alpha value to the below list if you want to run the algo\na_list = numpy.array([])\n\nfor alpha in a_list:\n    #Set the base model\n    model = Ridge(alpha=alpha,random_state=seed)\n    \n    algo = \"Ridge\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % alpha )\n\n#Result obtained by running the algo for alpha=1.0    \nif (len(a_list)==0):\n    mae.append(1267.5)\n    comb.append(\"Ridge\" + \" %s\" % 1.0 )    \n    \n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1267 with alpha=1",
   "execution_count": 13,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* LASSO Linear Regression (Linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of Lasso LinearRegression\n\n#Import the library\nfrom sklearn.linear_model import Lasso\n\n#Add the alpha value to the below list if you want to run the algo\na_list = numpy.array([])\n\nfor alpha in a_list:\n    #Set the base model\n    model = Lasso(alpha=alpha,random_state=seed)\n    \n    algo = \"Lasso\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % alpha )\n\n#Result obtained by running the algo for alpha=0.001    \nif (len(a_list)==0):\n    mae.append(1262.5)\n    comb.append(\"Lasso\" + \" %s\" % 0.001 )\n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#High computation time\n#Best estimated performance is 1262.5 for alpha = 0.001",
   "execution_count": 14,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Elastic Net Regression (Linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of ElasticNet LinearRegression\n\n#Import the library\nfrom sklearn.linear_model import ElasticNet\n\n#Add the alpha value to the below list if you want to run the algo\na_list = numpy.array([])\n\nfor alpha in a_list:\n    #Set the base model\n    model = ElasticNet(alpha=alpha,random_state=seed)\n    \n    algo = \"Elastic\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % alpha )\n\nif (len(a_list)==0):\n    mae.append(1260)\n    comb.append(\"Elastic\" + \" %s\" % 0.001 )\n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#High computation time\n#Best estimated performance is 1260 for alpha = 0.001",
   "execution_count": 15,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* KNN (non-linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of KNN\n\n#Import the library\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#Add the N value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_neighbors in n_list:\n    #Set the base model\n    model = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\n    \n    algo = \"KNN\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_neighbors )\n\nif (len(n_list)==0):\n    mae.append(1745)\n    comb.append(\"KNN\" + \" %s\" % 1 )\n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Very high computation time\n#Best estimated performance is 1745 for n=1",
   "execution_count": 16,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* CART (non-linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of CART\n\n#Import the library\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Add the max_depth value to the below list if you want to run the algo\nd_list = numpy.array([])\n\nfor max_depth in d_list:\n    #Set the base model\n    model = DecisionTreeRegressor(max_depth=max_depth,random_state=seed)\n    \n    algo = \"CART\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % max_depth )\n\nif (len(a_list)==0):\n    mae.append(1741)\n    comb.append(\"CART\" + \" %s\" % 5 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#High computation time\n#Best estimated performance is 1741 for depth=5",
   "execution_count": 17,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* SVM (Non-linear algo)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of SVM\n\n#Import the library\nfrom sklearn.svm import SVR\n\n#Add the C value to the below list if you want to run the algo\nc_list = numpy.array([])\n\nfor C in c_list:\n    #Set the base model\n    model = SVR(C=C)\n    \n    algo = \"SVM\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % C )\n\n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#very very high computation time",
   "execution_count": 18,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Bagged Decision Trees (Bagging)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of Bagged Decision Trees\n\n#Import the library\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\n    \n    algo = \"Bag\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#very high computation time",
   "execution_count": 19,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Random Forest (Bagging)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of RandomForest\n\n#Import the library\nfrom sklearn.ensemble import RandomForestRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n    \n    algo = \"RF\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1213)\n    comb.append(\"RF\" + \" %s\" % 50 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1213 when the number of estimators is 50",
   "execution_count": 20,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Extra Trees (Bagging)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of ExtraTrees\n\n#Import the library\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = ExtraTreesRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n    \n    algo = \"ET\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1254)\n    comb.append(\"ET\" + \" %s\" % 100 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1254 for 100 estimators",
   "execution_count": 21,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* AdaBoost (Boosting)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of AdaBoost\n\n#Import the library\nfrom sklearn.ensemble import AdaBoostRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\n    \n    algo = \"Ada\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1678)\n    comb.append(\"Ada\" + \" %s\" % 100 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1678 with n=100",
   "execution_count": 22,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* Stochastic Gradient Boosting (Boosting)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of SGB\n\n#Import the library\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\n    \n    algo = \"SGB\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1278)\n    comb.append(\"SGB\" + \" %s\" % 50 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is ?",
   "execution_count": 23,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* XGBoost",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of XGB\n\n#Import the library\nfrom xgboost import XGBRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n    \n    algo = \"XGB\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1169)\n    comb.append(\"XGB\" + \" %s\" % 1000 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1169 with n=1000",
   "execution_count": 24,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation, prediction, and analysis\n* MLP (Deep Learning)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Evaluation of various combinations of multi-layer perceptrons\n\n#Import libraries for deep learning\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# define baseline model\ndef baseline(v):\n     # create model\n     model = Sequential()\n     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n     model.add(Dense(1, init='normal'))\n     # Compile model\n     model.compile(loss='mean_absolute_error', optimizer='adam')\n     return model\n\n# define smaller model\ndef smaller(v):\n     # create model\n     model = Sequential()\n     model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n     model.add(Dense(1, init='normal', activation='relu'))\n     # Compile model\n     model.compile(loss='mean_absolute_error', optimizer='adam')\n     return model\n\n# define deeper model\ndef deeper(v):\n # create model\n model = Sequential()\n model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n model.add(Dense(1, init='normal', activation='relu'))\n # Compile model\n model.compile(loss='mean_absolute_error', optimizer='adam')\n return model\n\n# Optimize using dropout and decay\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout\nfrom keras.constraints import maxnorm\n\ndef dropout(v):\n    #create model\n    model = Sequential()\n    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, init='normal', activation='relu'))\n    # Compile model\n    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n    model.compile(loss='mean_absolute_error', optimizer=sgd)\n    return model\n\n# define decay model\ndef decay(v):\n    # create model\n    model = Sequential()\n    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n    model.add(Dense(1, init='normal', activation='relu'))\n    # Compile model\n    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n    model.compile(loss='mean_absolute_error', optimizer=sgd)\n    return model\n\nest_list = []\n#uncomment the below if you want to run the algo\n#est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n\nfor name, est in est_list:\n \n    algo = name\n\n    #Accuracy of the model using all features\n    for m,i_cols_list in X_all:\n        model = KerasRegressor(build_fn=est, v=1, nb_epoch=10, verbose=0)\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo )\n\nif (len(est_list)==0):\n    mae.append(1168)\n    comb.append(\"MLP\" + \" baseline\" )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n#Plot the MAE of all combinations\nfig, ax = plt.subplots()\nplt.plot(mae)\n#Set the tick names to names of combinations\nax.set_xticks(range(len(comb)))\nax.set_xticklabels(comb,rotation='vertical')\n#Plot the accuracy for all combinations\nplt.show()    \n\n#Best estimated performance is MLP=1168",
   "execution_count": 25,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Make Predictions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Make predictions using XGB as it gave the best estimated performance        \n\nX = numpy.concatenate((X_train,X_val),axis=0)\ndel X_train\ndel X_val\nY = numpy.concatenate((Y_train,Y_val),axis=0)\ndel Y_train\ndel Y_val\n\nn_estimators = 1000\n\n#Best model definition\nbest_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\nbest_model.fit(X,Y)\ndel X\ndel Y\n#Read test dataset\ndataset_test = pandas.read_csv(\"../input/test.csv\")\n#Drop unnecessary columns\nID = dataset_test['id']\ndataset_test.drop('id',axis=1,inplace=True)\n\n#One hot encode all categorical attributes\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(dataset_test.iloc[:,i])\n    feature = feature.reshape(dataset_test.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n\n# Make a 2D array from a list of 1D arrays\nencoded_cats = numpy.column_stack(cats)\n\ndel cats\n\n#Concatenate encoded attributes with continuous attributes\nX_test = numpy.concatenate((encoded_cats,dataset_test.iloc[:,split:].values),axis=1)\n\ndel encoded_cats\ndel dataset_test\n\n#Make predictions using the best model\npredictions = numpy.expm1(best_model.predict(X_test))\ndel X_test\n# Write submissions to output file in the correct format\nwith open(\"submission.csv\", \"w\") as subfile:\n    subfile.write(\"id,loss\\n\")\n    for i, pred in enumerate(list(predictions)):\n        subfile.write(\"%s,%s\\n\"%(ID[i],pred))",
   "execution_count": 26,
   "outputs": [],
   "metadata": {}
  }
 ]
}
